{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegisKonan/Computer_Vision/blob/main/regis_djaha_1b_softmax_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqnl0AKVXIA4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zod_L3iW-0gy"
      },
      "source": [
        "# Tutorial 1b: Softmax Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1DV-MS2bxYq"
      },
      "source": [
        "**Question:** To have the logistic regressor output probabilities, they need to be processed through a softmax layer. Implement a softmax layer yourself. What numerical issues may arise in this layer? How can you solve them? Use the testing code to confirm you implemented it correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0WI8kwo-0gz"
      },
      "outputs": [],
      "source": [
        "logits = torch.rand((1, 20)) + 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj4X2PnOfK9W"
      },
      "outputs": [],
      "source": [
        "def bad_softmax(x: Tensor) -> Tensor:\n",
        "    return torch.exp(x) / torch.sum(torch.exp(logits), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjlZxVYK-0g0",
        "outputId": "1757e14b-a747-4d02-f509-d9d582afc4e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(nan)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "torch.sum(bad_softmax(logits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OtfgG6j-0g0"
      },
      "outputs": [],
      "source": [
        "#def good_softmax(x: Tensor) -> Tensor:\n",
        "    ###########################################################################\n",
        "    # TODO: Implement a more stable way to compute softmax                    #\n",
        "    ###########################################################################\n",
        "   # return "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def good_softmax(x: Tensor):\n",
        "    z = x - max(x)\n",
        "    numerator = torch.exp(z)\n",
        "    denominator = torch.sum(numerator,axis=0)\n",
        "    softmax = numerator/denominator\n",
        "\n",
        "    return softmax"
      ],
      "metadata": {
        "id": "vb-loXjxCd7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6l1gouA-0g0",
        "outputId": "3566fd30-e260-4cea-c874-350dc3f03d5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(20.)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "torch.sum(good_softmax(logits))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C_J5S0RScXJ"
      },
      "source": [
        "Because of numerical issues like the one you just experiences, PyTorch code typically uses a `LogSoftmax` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgStX-ctjIms"
      },
      "source": [
        "**Question [optional]:** PyTorch automatically computes the backpropagation gradient of a module for you. However, it can be instructive to derive and implement your own backward function. Try and implement the backward function for your softmax module and confirm that it is correct."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Softmax(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        exps = torch.exp(x - torch.max(x))\n",
        "        return exps / torch.sum(exps)\n",
        "    \n",
        "    def backward(self, grad_output):\n",
        "        softmax = self.forward()\n",
        "        outer_product = torch.ger(softmax, softmax)\n",
        "        jacobian = torch.diag(softmax) - outer_product\n",
        "        return torch.matmul(grad_output, jacobian)"
      ],
      "metadata": {
        "id": "9o9mhPmilArL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = Softmax()\n",
        "x = torch.randn(10, 20) # input tensor of shape (10, 20)\n",
        "output = softmax.forward(x) # call the forward method with the input tensor\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yI9gxUhlLqe",
        "outputId": "d871d2ca-d00c-40e6-edce-6c0d80479a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0026, 0.0010, 0.0011, 0.0016, 0.0045, 0.0067, 0.0035, 0.0019, 0.0019,\n",
            "         0.0100, 0.0028, 0.0007, 0.0040, 0.0022, 0.0055, 0.0021, 0.0007, 0.0040,\n",
            "         0.0053, 0.0049],\n",
            "        [0.0022, 0.0014, 0.0040, 0.0025, 0.0055, 0.0018, 0.0057, 0.0066, 0.0150,\n",
            "         0.0102, 0.0034, 0.0023, 0.0164, 0.0039, 0.0002, 0.0011, 0.0003, 0.0068,\n",
            "         0.0018, 0.0118],\n",
            "        [0.0084, 0.0011, 0.0013, 0.0010, 0.0127, 0.0008, 0.0020, 0.0105, 0.0027,\n",
            "         0.0042, 0.0014, 0.0023, 0.0019, 0.0011, 0.0032, 0.0078, 0.0014, 0.0069,\n",
            "         0.0265, 0.0044],\n",
            "        [0.0143, 0.0044, 0.0016, 0.0026, 0.0063, 0.0090, 0.0031, 0.0018, 0.0185,\n",
            "         0.0008, 0.0065, 0.0487, 0.0123, 0.0012, 0.0038, 0.0003, 0.0028, 0.0068,\n",
            "         0.0011, 0.0155],\n",
            "        [0.0056, 0.0050, 0.0020, 0.0028, 0.0004, 0.0035, 0.0010, 0.0147, 0.0066,\n",
            "         0.0166, 0.0042, 0.0080, 0.0091, 0.0070, 0.0123, 0.0087, 0.0014, 0.0017,\n",
            "         0.0052, 0.0019],\n",
            "        [0.0047, 0.0013, 0.0065, 0.0020, 0.0003, 0.0014, 0.0028, 0.0005, 0.0102,\n",
            "         0.0075, 0.0016, 0.0016, 0.0056, 0.0149, 0.0023, 0.0033, 0.0009, 0.0058,\n",
            "         0.0010, 0.0010],\n",
            "        [0.0010, 0.0023, 0.0006, 0.0012, 0.0015, 0.0064, 0.0298, 0.0026, 0.0005,\n",
            "         0.0037, 0.0062, 0.0045, 0.0018, 0.0091, 0.0036, 0.0021, 0.0031, 0.0014,\n",
            "         0.0041, 0.0094],\n",
            "        [0.0056, 0.0020, 0.0012, 0.0027, 0.0018, 0.0033, 0.0041, 0.0039, 0.0048,\n",
            "         0.0021, 0.0015, 0.0019, 0.0013, 0.0035, 0.0029, 0.0014, 0.0043, 0.0019,\n",
            "         0.0066, 0.0069],\n",
            "        [0.0024, 0.0008, 0.0204, 0.0018, 0.0048, 0.0065, 0.0064, 0.0013, 0.0037,\n",
            "         0.0038, 0.0146, 0.0015, 0.0012, 0.0014, 0.0023, 0.0029, 0.0105, 0.0064,\n",
            "         0.0007, 0.0007],\n",
            "        [0.0041, 0.0015, 0.0004, 0.0042, 0.0021, 0.0122, 0.0314, 0.0107, 0.0044,\n",
            "         0.0073, 0.0065, 0.0011, 0.0018, 0.0064, 0.0002, 0.0051, 0.0114, 0.0013,\n",
            "         0.0030, 0.0060]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0V0VQb1YkkEQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "convnet_tutorials",
      "language": "python",
      "name": "convnet_tutorials"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}